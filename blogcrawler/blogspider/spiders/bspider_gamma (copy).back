import re
import csv
import time
import itertools as it
from pprint import pprint
from copy import deepcopy
from items import BlogItem
from fuzzywuzzy import fuzz
from nltk import clean_html
from scrapy.item import Item
from urlparse import urlparse
from scrapy.http import Request
from collections import OrderedDict
from scrapy.selector import Selector
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor


class MySpider(CrawlSpider):
	name = 'blogspider'
	DENY_PATTERNS = re.compile('(\/tag)|(\/search\/)|(\/category\/)|(\?tag\=)|(\/search\?max-results=)|(_archive\.html)|(\/search\?by-date=)|(\?showComment=)|(\?shared=)|(\?msg=)|(\?replytocom=)|(\/author\/)|(\/\d{4}\/\d{2}\/$)|(\/page\/)|(\/\?page=)')

	def __init__(self, blogurl='http://mycottoncreations.blogspot.com',blogid='7777'):
		self.start_urls = [blogurl,]
		self.allowed_domains = [self.get_domain(x) for x in self.start_urls]
		self.blog_ids = {blogurl: blogid}
		self.blog_id=blogid
		self.rules = (
			Rule(
				SgmlLinkExtractor(allow_domains=self.allowed_domains,deny_domains=['facebook.com','google.com','twitter.com','pintrest.com'],unique=True),
				process_request='add_meta',follow=True, callback='parse_item'),
		)
		super(MySpider, self).__init__()

	def get_link(self, blog):
		if 'http' not in blog:
			return "http://" + blog
		return blog

	def get_domain(self, blog):
		if 'http://' in blog:
			blog = blog.replace('http://', '')
		elif 'https://' in blog:
			blog = blog.replace('https://', '')
		if 'www.' in blog:
			blog = blog.replace('www.', '')
		blog = blog.split('/')[0]
		return blog
	
	def add_meta(self, request):
		request.meta['id'] = self.blog_id
		if not self.DENY_PATTERNS.search(request.url):
			#yield item
			return request
	
	def parse_item(self, response):
		sel = Selector(response)
		date=None
		date_meta = sel.xpath('//meta[contains(@property,"article:published_time")]/@content').extract()
		if date_meta:
			date=date_meta[0] if date_meta else None
		if not date:
			date_title = re.compile('\d+\/\d+\/\d+').findall(response.url)
			date=date_title[0] if date_title else None
		if not date:
			date_span_xpath = ' '.join(sel.xpath('//span/text()').extract())
			date_span = re.compile('\w+ \d+, \d+').findall(date_span_xpath)
			date = date_span[0] if date_span else None
		if not date:
			date_text_xpath = ' '.join(sel.xpath('//text()').extract())
			date_text = re.compile('\w+ \d{2} \d{4}').findall(date_text_xpath)
			if not date_text:
				date_text = re.compile('\w+ \d{2}, \d{4}').findall(date_text_xpath)
			if not date_text:
				date_text = re.compile('\d+\/\d+\/\d+').findall(date_text_xpath)

			date = date_text[0] if date_text else None
		"""
		if not date:
			date_text_xpath = ' '.join(sel.xpath('//text()').extract())
			date_text = re.compile('\w+ \d{2} \d{4}').findall(date_text_xpath)
			if not date_text:
				date_text = re.compile('\w+ \d{2}, \d{4}').findall(date_text_xpath)
			date = date_text[0] if date_text else None
		"""
		title = None
		title_xpath = None
		head_title = sel.xpath('//title/text()').extract()
		head_title = head_title[0] if head_title else None

		h1 = "//h1"
		h1a = "//h1/a"
		h2 = "//h2"
		h2a = "//h2/a"
		h3 = "//h3"
		h3a = "//h3/a"

		title_score = 0
		head_xpaths=[h1a,h1,h2a,h2,h3a,h3]
		title_lists=[sel.xpath(head+'//text()').extract() for head in head_xpaths]
		title_dict = OrderedDict(zip(head_xpaths,title_lists))

		for title_xpaths,title_list in title_dict.iteritems():
			if title_list:
				for titles in title_list:
					title_ratio = fuzz.partial_token_sort_ratio(titles,head_title)
					#slug_ratio to be added in case
					if title_ratio>title_score:
						title_score = title_ratio
						title = titles
						title_xpath = title_xpaths
						if title_score==100:
							break
				if title_score==100:
					break

		#check for max possible title
		if title_score<50:
			title = head_title

		if title and title_xpath:
			post_text = self.find_post_text(title_xpath,sel)
			title = ' '.join(title.split()).replace('&amp;','&') if title else None
			text = ' '.join(post_text.split()).replace('&amp;','&') if post_text else None

			date = date if date else None
			base_url = urlparse(response.url)
			item = BlogItem(blog_url=base_url.netloc,
							post_url=response.url,
							post_date=date,
							post_title=title,
							post_text=text,
							blog_id=response.meta['id'],
							)
			if not self.DENY_PATTERNS.search(item['post_url']) and item['post_text']:
				yield item


	def find_post_text(self,title_xpath,sel):
		post_text = ''
		post_text1 = ''
		div_len1 = 0
		div_html = ''
		div_text = ''
		post_xpaths1 = [title_xpath + "/following-sibling::div[1]", title_xpath +
					   "/following-sibling::div[2]", title_xpath + "/following-sibling::div[3]"]
		for post_xpath in post_xpaths1:
			div_html = sel.xpath(post_xpath).extract()
			div_text = clean_html(' '.join(div_html))

			if len(div_text) > div_len1:
				if len(re.compile('\w+ \d+,.\d+').findall(div_html[0])) > 10:
					continue
				else:
					post_text1 = div_text
					post_text1 = ' '.join(post_text1.split())
					div_len1 = len(div_text)

		# if post is in upper /sibling div
		post_text2 = ''
		div_len2 = 0
		div_html=''
		post_xpaths2 = [title_xpath + "/../following-sibling::div[1]", title_xpath +
						"/../following-sibling::div[2]", title_xpath + "/../following-sibling::div[3]"]
		post_div = ''
		for post_xpath in post_xpaths2:
			div_html = sel.xpath(post_xpath).extract()
			div_text = clean_html(' '.join(div_html))
			if len(div_text) > div_len2:
				if len(re.compile('\w+ \d+,.\d+').findall(div_html[0])) > 10:
					continue
				else:
					post_text2 = div_text
					post_text2 = ' '.join(post_text2.split())
					div_len2 = len(div_text)

		"""
		if len(' '.join(post_text2.split())) > len(' '.join(post_text.split())):
			post_text = post_text2
		"""
		# if no post is found in page and post is in 2nd upper div then
		post_text3 = ''
		div_len3 = 0
		div_html=''
		post_xpaths3 = [title_xpath + "/../../following-sibling::div[1]", title_xpath +
						"/following-sibling::div[2]", title_xpath + "/following-sibling::div[3]"]

		for post_xpath in post_xpaths3:
			div_html = sel.xpath(post_xpath).extract()
			div_text = clean_html(' '.join(div_html))

			if len(div_text) > div_len3:
				if len(re.compile('\w+ \d+,.\d+').findall(div_html[0])) > 10:
					continue
				else:
					post_text3 = div_text
					post_text3 = ' '.join(post_text3.split())
					div_len3 = len(div_text)

		pos=[' '.join(post_text1.split()),' '.join(post_text2.split())]

		post_text = max(pos,key=lambda p:len(p))
		if len(post_text3)>len(post_text):
			if post_text3.lower().count('comments')<=post_text.lower().count('comments'):
				post_text = post_text3
		return post_text
